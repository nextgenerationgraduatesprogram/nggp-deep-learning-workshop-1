{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NextGen2024**: Deep Learning Workshop 1 \n",
    "\n",
    "Within these workshops, we will dive into the learning problem, and explore what control we have as practitioners. Understanding the balance and trade-offs between different variables can provide rich insight into the design and function of modern deep learning systems and directions.\n",
    "\n",
    "We will explore this within the context of some simple models and toy datasets before moving onto explore these within the context of more modern recent architectures during our next workshop.\n",
    "\n",
    "Through these workshops I hope to convey a better understanding of the fundamental trade-offs in the design and implementation of deep learning systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from typing import *\n",
    "from torch import Tensor\n",
    "\n",
    "# seed training for deterministic results\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Section 1] Function Approximation\n",
    "\n",
    "Informally a function is just a system of outputs, numbers in $x$, numbers out $y=f(x)$. If you know the function you can always calculate the correct output $y$ for a given input $x$.\n",
    "\n",
    "<img src=\"Linear.png\" alt=\"Computer Vision\" width=\"300\"/>\n",
    "\n",
    "We can formulate nearly any task as a function. Consider object classification in computer vision, given a set of pixel values $x \\in \\mathbb{R}^{H \\times W \\times C}$ as input, we want to output a set of probabilities representing the likelihood a class exists in an image $y \\in \\mathbb{R}^{N_{c}}$. Say we have some target function $f$ which can perfectly perform this task, then for any image we can calculate which classes exists in that image..\n",
    "\n",
    "<img src=\"ObjectClassificationDetection.jpg\" alt=\"Computer Vision\" width=\"600\"/>\n",
    "\n",
    "In the case of a simple linear relationship this function is more obvious $f(x) = mx + c$. However in the case of object classification, the function $f$ that maps the input pixels to the output classes probabilities is a little more difficult. So what happens if we don't know the function?\n",
    "\n",
    "Fundamentally this is what machine learning and deep-learning is about. Given some observations $x$ and $y=f(x)$ from a data generating process, how can we learning the function $f$ that represents or atleast approximates this process.\n",
    "\n",
    "Let's begin by exploring a simple single dimensional perceptron..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function(x):\n",
    "    \"\"\" Data generating process.\n",
    "    \"\"\"\n",
    "    return 2 * x + 3\n",
    "\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    \"\"\" Perceptron model\n",
    "\n",
    "    y = Wx + B\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, use_activation=False):\n",
    "        super(Perceptron, self).__init__()\n",
    "\n",
    "        # Create linear layer\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "        self.fc = nn.Linear(1, 1)\n",
    "        self.fc.weight = nn.Parameter(torch.tensor([[1.0]])) # TODO: Modify these values to fit the target function.\n",
    "        self.fc.bias = nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "        # Activation Function\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "        self.activation = nn.ReLU() if use_activation else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc(x)) if self.activation is not None else self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the inputs x\n",
    "x = torch.linspace(-10, 10, 100)\n",
    "\n",
    "# Use the target function to generate the outputs\n",
    "y = target_function(x)\n",
    "\n",
    "# Create the model and get the predicted values\n",
    "with torch.no_grad(): \n",
    "    y_p = Perceptron(use_activation=False)(x.unsqueeze(-1)) # TODO: See what happens if you use the activation function.\n",
    "    \n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x, y, alpha=0.75, label=\"Target f(x)\")\n",
    "ax.scatter(x, y_p, alpha=0.75, label=\"Predicted f_approx(x)\")\n",
    "ax.grid(True, alpha=0.25)\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with a single perceptron we can approximate a linear relationship, atleast when we're not using any activation function. However, when we have a slighlty more complex data generating process say $f(x) = 0.5x^{2} - 20$, we end up needing to use more perceptrons - this is akin to using multiple piecewise functions to approximate $f$. In this scenario using non-linearity is essential to be able to approximate this non-linear function.\n",
    "\n",
    "See if you can hand pick the values of the two perceptrons to approximate the function $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function(x):\n",
    "    \"\"\" Data generating process.\n",
    "    \"\"\"\n",
    "    return 0.5 * x**2 - 20\n",
    "\n",
    "\n",
    "class TwoPerceptron(nn.Module):\n",
    "    \"\"\" Perceptron model\n",
    "\n",
    "    y = ReLU(W1*x + B2) + ReLU(W2*x + B2)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TwoPerceptron, self).__init__()\n",
    "\n",
    "        # Create Single Perceptron\n",
    "        self.fc1 = nn.Linear(1, 1)\n",
    "        self.fc1.weight = nn.Parameter(torch.tensor([[...]])) # TODO: Fill in these values e.g. 1.0\n",
    "        self.fc1.bias = nn.Parameter(torch.tensor([...]))\n",
    "\n",
    "        # Create Single Perceptron\n",
    "        self.fc2 = nn.Linear(1, 1)\n",
    "        self.fc2.weight = nn.Parameter(torch.tensor([[...]]))\n",
    "        self.fc2.bias = nn.Parameter(torch.tensor([...]))\n",
    "\n",
    "        # Activation Function\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x)) + self.activation(self.fc2(x)) - ... # TODO: Add in some offset\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the inputs x\n",
    "x = torch.linspace(-10, 10, 100)\n",
    "\n",
    "# Define the outputs y\n",
    "y = target_function(x)\n",
    "\n",
    "# Create the model and get the predicted values\n",
    "with torch.no_grad():\n",
    "    y_p = TwoPerceptron()(x.unsqueeze(-1))\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x, y, alpha=0.75, label=\"Target f(x)\")\n",
    "ax.scatter(x, y_p, alpha=0.75, label=\"Predicted f_approx(x)\")\n",
    "ax.grid(True, alpha=0.25)\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just two perceptrons we can see that we do a pretty poor job of approximating $f$, if we increase the number of perceptrons we can better approximate $f$, however it becomes increasingly more prohibitive to tune as the number of parameters increases.\n",
    "\n",
    "We could also consider stacking layers of these perceptrons to create a multi-layer perceptron, this would let us model increasingly complex and non-linear functions. Multi-layer perceptrons are provably universal function approximators [Multilayer feedforward networks are universal approximators](https://www.sciencedirect.com/science/article/pii/0893608089900208?via%3Dihub), that is they can approximate any function to any degree of precision within a bounded domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### [Section 1A] Classification\n",
    "We can use neural networks to approximate functions, lets see how we can apply this in a classification example.\n",
    "\n",
    "Consider two clusters sitting on the $xy$-plane, each of the points $P = (x, y)$ is associated with a cluster. We want to map each point $P$ to a cluster.  We can do this by mapping each input 2-D point $(x,y)$ to an output 1-D number $z$. Lets say we want this output number to be negative ($z<0$) if the cluster is orange and positive ($z>0$) if the cluster is blue.\n",
    "\n",
    "<img src=\"Clusters.png\" alt=\"Clusters\" width=\"600\"/>\n",
    "\n",
    "We can do this by re-defining our perceptron model from before to use 2 inputs instead of 1. Where as before our perceptron was representing a line $y = F(x)$, it's now representing a plane $z = F(x, y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Clusters\n",
    "mu = torch.zeros((50))\n",
    "std = torch.ones((50))\n",
    "c1_x = torch.normal(mu, std) + 5\n",
    "c1_y = torch.normal(mu, std) + 5\n",
    "c1_z = torch.zeros((50))\n",
    "c2_x = torch.normal(mu, std) - 5\n",
    "c2_y = torch.normal(mu, std) - 5\n",
    "c2_z = torch.zeros((50))\n",
    "\n",
    "# Create a simple perceptron model\n",
    "\"\"\" #TODO: We've re-defined the perceptron to use 2-inputs.\n",
    "\"\"\"\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc = nn.Linear(2, 1) # NOTE: Change here from 1 input to 2.\n",
    "        self.fc.weight = nn.Parameter(torch.tensor([[..., ...]])) # TODO: Set some values here to model a plane $f(x,y) = ax + by + c\n",
    "        self.fc.bias = nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Evaluate the perceptron on a grid\n",
    "plane_xy = torch.zeros(100, 100, 3)\n",
    "model_results = torch.zeros(100, 100, 3)\n",
    "x_1 = torch.linspace(-10, 10, 100)\n",
    "x_2 = torch.linspace(-10, 10, 100)\n",
    "model = Perceptron()\n",
    "for idx, x1 in enumerate(x_1):\n",
    "    for jdx, x2 in enumerate(x_2):\n",
    "        model_results[idx,jdx,0] = x1\n",
    "        model_results[idx,jdx,1] = x2\n",
    "        with torch.no_grad(): model_results[idx,jdx,2] = model(torch.tensor([x1, x2]))\n",
    "        plane_xy[idx, jdx, 0] = x1\n",
    "        plane_xy[idx, jdx, 1] = x2\n",
    "        plane_xy[idx, jdx, 2] = 0\n",
    "plane_xy = plane_xy.reshape(100*100,3)   \n",
    "model_results = model_results.reshape(100*100, 3)\n",
    "\n",
    "# Evaluate the perceptron for the cluster points\n",
    "proj_c1_z = torch.zeros(50)\n",
    "proj_c2_z = torch.zeros(50)\n",
    "for idx in range(50):\n",
    "    # Cluster 1/2\n",
    "    with torch.no_grad(): proj_c1_z[idx] = model(torch.tensor([c1_x[0], c1_y[0]]))\n",
    "    with torch.no_grad(): proj_c2_z[idx] = model(torch.tensor([c2_x[0], c2_y[0]]))\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(plane_xy[:,0], plane_xy[:,1], plane_xy[:,2], color=\"tab:gray\", alpha=0.01, label=\"XY-plane\")\n",
    "ax.scatter(c1_x, c1_y, c1_z, color=\"tab:blue\", alpha=0.75, label=\"Cluster 1\")\n",
    "ax.scatter(c2_x, c2_y, c2_z, color=\"tab:orange\", alpha=0.75, label=\"Cluster 2\")\n",
    "ax.grid(True, alpha=0.25)\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "ax.set_xlim(-10, 10)\n",
    "ax.set_ylim(-10, 10)\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(plane_xy[:,0], plane_xy[:,1], plane_xy[:,2], color=\"tab:gray\", alpha=0.01, label=\"XY-plane\")\n",
    "ax.scatter(model_results[:,0], model_results[:,1], model_results[:,2], color=\"tab:olive\", alpha=0.05, label=\"Model\")\n",
    "ax.scatter(c1_x, c1_y, proj_c1_z, color=\"tab:blue\", alpha=0.75, label=\"Projected Cluster 1\")\n",
    "ax.scatter(c2_x, c2_y, proj_c2_z, color=\"tab:orange\", alpha=0.75, label=\"Projected Cluster 2\")\n",
    "ax.scatter(c1_x, c1_y, c1_z, color=\"tab:gray\", alpha=0.75)\n",
    "ax.scatter(c2_x, c2_y, c2_z, color=\"tab:gray\", alpha=0.75)\n",
    "ax.grid(True, alpha=0.25)\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "ax.set_xlim(-10, 10)\n",
    "ax.set_ylim(-10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've used our Perceptron model to calculate the height $z$ for each poihnt $P = (x, y)$. We can see that the orange cluster has negative z-values and the blue cluster has positive z-values. Let's say our decision boundary is $z=0$, anything with $z<0$ will belong to the orange cluster and anything with $z>0$ with belong to the blue cluster. Within this example, we have mapped a 2-D input to a 1-D output, this makes it much easier to draw a decision boundary for our classification problem.\n",
    "\n",
    "<img src=\"Clusters.png\" alt=\"Clusters\" width=\"600\"/>\n",
    "\n",
    "Within deep-learning we generally deal with much higher dimensional and more complex inputs and models, but the general idea remains the same, we want our model to learn to transform some input space $\\mathcal{R}^{in}$ into some useful output space $\\mathcal{R}^{out}$. But the problem now requires some more complex and abstract function for our network to learn to approximate.\n",
    "\n",
    "Okay, that sounds a bit more useful, why don't we use multi-layer perceptrons for everything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## [Section 2] The Learning Problem\n",
    "\n",
    "In reality learning to approximate a function is a balance between the model, dataset and optimization process.\n",
    "\n",
    "\n",
    "**The Hypothesis Space**: Lets refer to the set of possible models obtained from the training process as the hypothesis space $\\mathcal{H}$. We'll also denote the set of possible solutions as the solution space $\\mathcal{S}$.\n",
    "\n",
    "<img src=\"Slide1.JPG\" alt=\"Slide 1\" width=\"600\"/>\n",
    "\n",
    "\n",
    "In machine learning, the hypothesis space $\\mathcal{H}$ is typically determine by of our choice of model $\\mathcal{H}_{\\theta}$, data $\\mathcal{H}_{D}$, and optimization $\\mathcal{H}_{o}$ spaces.\n",
    "- A model can generally never truly represent the lower dimensional rules governing a system.\n",
    "- A dataset can generally never truly fully represent in a complexity balanced manner the distribution of interest.\n",
    "- A optimization process may never find the true global minima in the error landscape.\n",
    "\n",
    "<img src=\"Slide2.JPG\" alt=\"Slide 2\" width=\"600\"/>\n",
    "\n",
    "\n",
    "**Approximation Error**: The approximation error $\\epsilon_{app}$ is the error between our target function $f$ the best possible model $h^{*} \\in \\mathcal{H}$. This error represents the *BEST* performance we can get out of our hypothesis space.\n",
    "\n",
    "**Generalization Error**: The generalization error $\\epsilon_{gen}$ is the error between our target function $f$ and our trained model $h \\in \\mathcal{H}$. This error represents the *ACTUAL* performance obtained as a result of the learning process.\n",
    "\n",
    "**Estimation Error**: The estimation error $\\epsilon_{est}$ is the difference in error between our trained model $h$ and the optimal model $h^{*}$.\n",
    "\n",
    "<img src=\"Slide3.JPG\" alt=\"Slide 3\" width=\"600\"/>\n",
    "\n",
    "Within this workshop we will explore how the **model** and **dataset** interact and affect the outcome of the learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## [Section 3] Spiral Classification\n",
    "\n",
    "Let's say we have collected some data from some physical system that produces a set of spirals, and we're interested in classifying the different spiral arms generated.\n",
    "\n",
    "<img src=\"TwoSpirals.jpg\" alt=\"Spiral\" width=\"300\"/>\n",
    "\n",
    "Thankfully it so happens that the data generated follows a fairly simple target function $r(\\theta) = \\alpha + \\beta * \\theta + \\mathcal{N}(\\mu,\\sigma)$, in reality with deep-learning related tasks we rarely know or can even represent this target function.\n",
    "\n",
    "Let us see what data we've managed to collect first..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data**\n",
    "\n",
    "We'll generate some data following this rule, and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets define our data generating process!\n",
    "\"\"\"\n",
    "def generateSpiralDataset(\n",
    "        spiral_num_samples: int,\n",
    "        spiral_noise: Optional[float] = 1., \n",
    "        spiral_revolutions: Optional[float] = 1,\n",
    "    ):\n",
    "        \"\"\" Lets use two simple archimedes spirals going in opposite directions using the\n",
    "        following rule: radius = alpha * theta\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [N * 3] where the columns contain [x-value, y-value, label] \n",
    "\n",
    "        \"\"\"\n",
    "        # Theta \n",
    "        theta = 2 * np.pi * spiral_revolutions * np.sqrt(np.random.rand(spiral_num_samples))\n",
    "\n",
    "        # Spiral A\n",
    "        radius_A = 2 * theta + np.pi / 2 # radius increases linearly with theta - anti-clockwise\n",
    "        x_A = radius_A * np.cos(theta) # x-component\n",
    "        y_A = radius_A * np.sin(theta) # y-component\n",
    "        spiral_A_data = np.array([x_A, y_A]).T # pack them together: column 0 = x-values, column 1 = y-values\n",
    "        spiral_A_data += spiral_noise * np.random.randn(spiral_num_samples,2) # add some noise\n",
    "        spiral_A_lbls = np.zeros((spiral_num_samples,1)) # spiral A = 0\n",
    "        spiral_A = np.append(spiral_A_data, spiral_A_lbls, axis=1)\n",
    "            \n",
    "        # Spiral B\n",
    "        radius_B = -2 * theta - np.pi / 2 # clockwise\n",
    "        x_B = radius_B * np.cos(theta)\n",
    "        y_B = radius_B * np.sin(theta)\n",
    "        spiral_B_data = np.array([x_B, y_B]).T\n",
    "        spiral_B_data += spiral_noise * np.random.randn(spiral_num_samples,2) \n",
    "        spiral_B_lbls = np.ones((spiral_num_samples,1)) # spiral B = 1\n",
    "        spiral_B = np.append(spiral_B_data, spiral_B_lbls, axis=1)\n",
    "\n",
    "        # Put the data into one big array\n",
    "        spirals = np.append(spiral_A, spiral_B, axis=0)\n",
    "        spirals = torch.from_numpy(spirals).to(dtype=torch.float32)\n",
    "\n",
    "        return spirals\n",
    "\n",
    "# Generate the spiral dataset\n",
    "data = generateSpiralDataset(\n",
    "    spiral_num_samples = 100, \n",
    "    spiral_noise = 0.5, \n",
    "    spiral_revolutions = 1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets plot the dataset, if possible using a color for each spiral...\n",
    "\"\"\" \n",
    "# Begin by separating the dataset into the different spirals for plotting...\n",
    "spiral_A_x = data[data[:,2] == 0][:,0]\n",
    "spiral_A_y = data[data[:,2] == 0][:,1]\n",
    "spiral_B_x = data[data[:,2] == 1][:,0]\n",
    "spiral_B_y = data[data[:,2] == 1][:,1]\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(spiral_A_x, spiral_A_y, alpha=0.75, label=\"Spiral A\")\n",
    "ax.scatter(spiral_B_x, spiral_B_y, alpha=0.75, label=\"Spiral B\")\n",
    "ax.set_title(\"Spiral Dataset\")\n",
    "ax.grid(True, alpha=0.25)\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset**\n",
    "\n",
    "Looks good!\n",
    "\n",
    "We decide to split up our dataset into training and testing sets, and create a dataset to draw samples from during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Split up our data into training and testing!\n",
    "\"\"\"\n",
    "class SpiralDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: Tensor, batch_size: Optional[int] = None):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size if batch_size is not None else self.data.shape[0]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.data.shape[0] // self.batch_size\n",
    "\n",
    "    def __iter__(self) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\" Yield data from the dataset in batches.\n",
    "        \"\"\" \n",
    "        n_samples = self.__len__()\n",
    "        for idx in range(n_samples):\n",
    "            # Extract data for current batch \n",
    "            sample = self.data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "\n",
    "            # Define inputs: x/y-values\n",
    "            x = sample[:,0:-1]\n",
    "\n",
    "            # Define targets: labels\n",
    "            y = sample[:,-1].unsqueeze(-1)\n",
    "\n",
    "            yield x, y\n",
    "\n",
    "\n",
    "# Create a function for splitting the data and creating the datasets\n",
    "def split_and_create_datasets(data: Tensor, training_fraction: Optional[float] = 0.7) -> Tuple[SpiralDataset, SpiralDataset]:\n",
    "    # Define the number of samples to use during training vs. testing\n",
    "    n_train = int(training_fraction * data.shape[0])\n",
    "    n_test = data.shape[0] - n_train\n",
    "\n",
    "    # Split the dataset into training/testing set\n",
    "    train_idxs, valid_idxs = tuple([s.indices for s in random_split(range(data.shape[0]), [n_train, n_test])])\n",
    "    train_data = data[train_idxs]\n",
    "    test_data = data[valid_idxs]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = SpiralDataset(train_data)\n",
    "    test_dataset = SpiralDataset(test_data)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "# Split data and create datasets\n",
    "train_dataset, test_dataset = split_and_create_datasets(data, training_fraction=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets plot the results\n",
    "\"\"\"\n",
    "# Create a function for plotting the spirals\n",
    "def plot_spirals_splits(train_data, test_data):\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.scatter(train_data[train_data[:,2] == 0,0], train_data[train_data[:,2] == 0,1], alpha=0.15, color=\"tab:blue\", label=\"Spiral A: Train\")\n",
    "    ax.scatter(train_data[train_data[:,2] == 1,0], train_data[train_data[:,2] == 1,1], alpha=0.15, color=\"tab:orange\", label=\"Spiral B: Train\")\n",
    "    ax.scatter(test_data[test_data[:,2] == 0,0], test_data[test_data[:,2] == 0,1], color=\"tab:blue\", label=\"Spiral A: Test\")\n",
    "    ax.scatter(test_data[test_data[:,2] == 1,0], test_data[test_data[:,2] == 1,1], color=\"tab:orange\", label=\"Spiral B: Test\")\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    ax.set_title(f\"Spiral Dataset\")\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "# Plot the spirals\n",
    "plot_spirals_splits(train_dataset.data, test_dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model**\n",
    "\n",
    "We decide to take a stab at modelling this dataset using a multi-layer perceptron, let's consider what we're trying to do...\n",
    "\n",
    "We decide that for each point $P=(x,y)$ we want to calculate the height $z$, where the height $z$ can be used to represent the class of each spiral. Lets say we want $z<0.5$ to correspond to Spiral $A$ and $z>0.5$ to correspond to Spiral $B$.\n",
    "\n",
    "<img src=\"2DSurface.png\" alt=\"Surface\" width=\"600\"/>\n",
    "\n",
    "By using multiple-layers of perceptrons we're hoping to approximate not just a plane as before but a more complex and non-linear 2D surface, through projecting and re-projecting into higher dimensional manifolds within the hidden layers, however the fundamental idea remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets define the architecture of our model!\n",
    "\"\"\"\n",
    "class Perceptron(nn.Module):\n",
    "    \"\"\" Create a single-layer perceptron with ReLU activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        input_dim: int, \n",
    "        output_dim: int\n",
    "    ):\n",
    "        super(Perceptron, self).__init__()\n",
    "\n",
    "        # Create linear layer\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "        # Create non-linear activation function\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \"\"\" Create a multi-layer perceptron by stacking multiple layers of perceptrons.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        input_dim: Optional[int] = 2, \n",
    "        output_dim: Optional[int] = 1, \n",
    "        hidden_layers: Optional[int] = 0, \n",
    "        hidden_dim: Optional[int] = 4\n",
    "    ):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        # Create input layer\n",
    "        modules = [Perceptron(input_dim, hidden_dim)]\n",
    "\n",
    "        # Create hidden layers\n",
    "        for _ in range(hidden_layers):\n",
    "            modules.append(Perceptron(hidden_dim, hidden_dim))\n",
    "\n",
    "        # Create output layer\n",
    "        modules.append(Perceptron(hidden_dim, output_dim))\n",
    "\n",
    "        # Create the model\n",
    "        self.layers = nn.ModuleList(modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training**\n",
    "\n",
    "We have our **dataset** and **model**, lets train up our spiral classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Next we're going to create the model, optimizer, loss function we want to train!\n",
    "\"\"\"\n",
    "# device\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# model\n",
    "model = MultiLayerPerceptron(input_dim=2, output_dim=1, hidden_layers=1, hidden_dim=4).to(device)\n",
    "\n",
    "# loss function\n",
    "loss_fn = torch.nn.MSELoss() # there are better loss functions but we'll use this for now\n",
    "\n",
    "# optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets train our model!\n",
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_epoch(dataset: Callable, model: Callable, loss_fn: Callable, opt: Callable):\n",
    "  \"\"\" Performs a training epoch.\n",
    "  \"\"\"\n",
    "  losses = []\n",
    "  for idx, (x, y) in enumerate(dataset):\n",
    "    # move data to device\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # forwards pass\n",
    "    y_p = model(x)\n",
    "\n",
    "    # compute the loss\n",
    "    l = loss_fn(y_p, y)\n",
    "\n",
    "    # performance the backwards pass (compute gradients)\n",
    "    l.backward()\n",
    "\n",
    "    # perform step of the optimizer and clear gradients\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # append loss\n",
    "    losses.append(l.item())\n",
    "\n",
    "  return losses, model\n",
    "\n",
    "\n",
    "def test_epoch(dataset: Callable, model: Callable, loss_fn: Callable):\n",
    "  \"\"\" Performs a testing epoch.\n",
    "  \"\"\"\n",
    "  losses = []\n",
    "  with torch.no_grad():\n",
    "    for idx, (x, y) in enumerate(dataset):\n",
    "      # move data to device\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # forwards pass\n",
    "      y_p = model(x)\n",
    "\n",
    "      # compute the loss\n",
    "      l = loss_fn(y_p, y)\n",
    "\n",
    "      # append loss\n",
    "      losses.append(l.item())\n",
    "    \n",
    "  return losses\n",
    "\n",
    "\n",
    "# Let's run the training loop!\n",
    "num_epochs = 1000\n",
    "train_losses = []\n",
    "valid_losses = {}\n",
    "train_loss, valid_loss = .0, .0\n",
    "with tqdm(range(num_epochs)) as pbar:\n",
    "  for epoch in pbar:\n",
    "    # Training\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses, model = train_epoch(train_dataset, model, loss_fn, opt)\n",
    "    train_losses += _losses\n",
    "    train_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "    # Validation\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses = test_epoch(test_dataset, model, loss_fn)\n",
    "    valid_losses[epoch*len(train_dataset)] = _losses[0]\n",
    "    valid_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets plot the training performance using the losses\n",
    "\"\"\"\n",
    "# Plot losses from training\n",
    "def plot_training_performance(train_losses, valid_losses):\n",
    "  fig = plt.figure(figsize=(16,4))\n",
    "  ax = fig.add_subplot(111)\n",
    "  ax.plot(train_losses, linestyle=\"-\", color=\"tab:blue\", label=\"Train\", alpha=0.75)\n",
    "  ax.plot(valid_losses.keys(), valid_losses.values(), linestyle=\"-\", color=\"tab:orange\", label=\"Valid\", alpha=0.75)\n",
    "  ax.set_xlim(left=0, right=len(train_losses))\n",
    "  ax.set_ylim(bottom=0, top=1)\n",
    "  ax.set_xlabel(\"Iterations\")\n",
    "  ax.set_ylabel(\"Loss\")\n",
    "  ax.set_title(\"Training\")\n",
    "  ax.legend(loc=\"best\")\n",
    "  ax.grid(True, alpha=0.25)\n",
    "\n",
    "\n",
    "# Plot \n",
    "plot_training_performance(train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets also check out how the model is making decisions over the input space.\n",
    "\"\"\"\n",
    "def plot_model_input_space(model, train_dataset, test_dataset, x_min, x_max, x_res: Optional[int] = 120):\n",
    "    # Create grid points\n",
    "    X1 = np.linspace(x_min, x_max, x_res).astype(np.float32)\n",
    "    X2 = np.linspace(x_min, x_max, x_res).astype(np.float32)\n",
    "\n",
    "    # Re-order the results \n",
    "    bg = np.zeros((x_res, x_res))\n",
    "    for idx, x1 in enumerate(X1):\n",
    "        for jdx, x2 in enumerate(X2):\n",
    "            # evaluate model at point\n",
    "            point = torch.tensor([[x1, x2]])\n",
    "            with torch.no_grad():\n",
    "                y_p = model(point.to(device)).cpu()\n",
    "            \n",
    "            # store result at point\n",
    "            bg[jdx, idx] = y_p.item()\n",
    "\n",
    "    # Clip values\n",
    "    bg[bg < 0] = .0\n",
    "    bg[bg > 1] = 1.\n",
    "\n",
    "    # Define normalization bounds \n",
    "    from matplotlib.colors import Normalize\n",
    "    norm = Normalize(vmin=0, vmax=1)\n",
    "\n",
    "    # Lets create a custom colorm\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    colors = [\"tab:orange\", \"white\", \"tab:blue\"]\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n",
    "\n",
    "    # Map the values to the colour map\n",
    "    bg_colors = cmap(bg)[::-1,::-1]\n",
    "\n",
    "    # Plot the results\n",
    "    from matplotlib.cm import ScalarMappable\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(bg_colors, extent=[x_min, x_max, x_min, x_max], origin=\"lower\", aspect=\"auto\", alpha=0.25)\n",
    "    fig.colorbar(ScalarMappable(norm=norm, cmap=cmap), ax=ax, label='Value')\n",
    "\n",
    "    # Lets also overlay the training and testing data\n",
    "    ax.scatter(train_dataset.data[train_dataset.data[:,2] == 0,0], train_dataset.data[train_dataset.data[:,2] == 0,1], alpha=0.15, color=\"tab:blue\", label=\"Spiral A: Train\")\n",
    "    ax.scatter(train_dataset.data[train_dataset.data[:,2] == 1,0], train_dataset.data[train_dataset.data[:,2] == 1,1], alpha=0.15, color=\"tab:orange\", label=\"Spiral B: Train\")\n",
    "    ax.scatter(test_dataset.data[test_dataset.data[:,2] == 0,0], test_dataset.data[test_dataset.data[:,2] == 0,1], color=\"tab:blue\", label=\"Spiral A: Test\")\n",
    "    ax.scatter(test_dataset.data[test_dataset.data[:,2] == 1,0], test_dataset.data[test_dataset.data[:,2] == 1,1], color=\"tab:orange\", label=\"Spiral B: Test\")\n",
    "\n",
    "    # Other figure stuff\n",
    "    ax.set_title(\"Network Outputs\")\n",
    "    ax.set_xlim(left=x_min, right=x_max)\n",
    "    ax.set_ylim(bottom=x_min, top=x_max)\n",
    "    ax.set_xlabel(\"x_1\")\n",
    "    ax.set_ylabel(\"x_2\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.grid(True, alpha=0.25)\n",
    "\n",
    "\n",
    "# Plot the values over the input space\n",
    "plot_model_input_space(model, train_dataset, test_dataset, x_min=-15, x_max=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### [Section 3A] **Model Bias-Complexity Trade-off**\n",
    "\n",
    "As we increase the size and complexity of our hypothesis class $\\mathcal{H}$ (i.e. creating a larger and higher capacity model), the approximation error $\\epsilon_{app}$ generally *decreases* as our best possible model becomes more capable of fitting the target function $f$. \n",
    "\n",
    "However, the estimation error $\\epsilon_{est}$ **may** also *increase* as the model becomes more capable of over-fitting the data distribution seen during training and not learning the lower dimensional rules governing the data generation process.\n",
    "\n",
    "<img src=\"Slide5.JPG\" alt=\"Slide 5\" width=\"600\"/>\n",
    "<img src=\"Slide6.JPG\" alt=\"Slide 6\" width=\"600\"/>\n",
    "\n",
    "Our learning problem has a fixed complexity, we need to find the balance between under-fitting and over-fitting the training distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss and optimizer\n",
    "model = MultiLayerPerceptron(\n",
    "  input_dim = 2,\n",
    "  output_dim = 1,\n",
    "  hidden_layers = 2,\n",
    "  hidden_dim = 8\n",
    ").to(device)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Let's run the training loop!\n",
    "num_epochs = 1000\n",
    "train_losses = []\n",
    "valid_losses = {}\n",
    "train_loss, valid_loss = .0, .0\n",
    "with tqdm(range(num_epochs)) as pbar:\n",
    "  for epoch in pbar:\n",
    "    # Training\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses, model = train_epoch(train_dataset, model, loss_fn, opt)\n",
    "    train_losses += _losses\n",
    "    train_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "    # Validation\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses = test_epoch(test_dataset, model, loss_fn)\n",
    "    valid_losses[epoch*len(train_dataset)] = _losses[0]\n",
    "    valid_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "# Plot the training performance\n",
    "plot_training_performance(train_losses, valid_losses)\n",
    "\n",
    "# Plot the values over the input space\n",
    "plot_model_input_space(model, train_dataset, test_dataset, x_min=-15, x_max=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### [Section 3B] **Dataset Bias-Complexity Trade-off**\n",
    "\n",
    "However, we also have to consider the other aspects of the learning problem, the complexity of the distribution also plays a role in defining the hypothesis class $\\mathcal{H}$. As we increase the size and complexity of the distribution space $\\mathcal{H}_{D}$ we generally expect our approximation error $\\epsilon_{app}$ to increase for a fixed model. The flip-side of this is also generally true.\n",
    "\n",
    "Peferably, we would collect a distribution that is fully representative of the rules which govern the data generation process, however this is generally not feasible. We typically like to make sure we have a *good enough* representation with respect to the distribution we expect to see during deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we decide to apply our model to another similar problem! Hey this binary star system has spirals, let's assume the spirals are the same as before but just extend for longer.\n",
    "\n",
    "<img src=\"CelestialSpiral.jpg\" alt=\"Spiral\" width=\"300\"/>\n",
    "\n",
    "Let's generate this dataset and see how well our previous model fits this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the spiral data : but longer...\n",
    "data = generateSpiralDataset(\n",
    "    spiral_num_samples = 400, \n",
    "    spiral_noise = 0.5, \n",
    "    spiral_revolutions = 2.0\n",
    ")\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset, test_dataset = split_and_create_datasets(data, training_fraction=0.7)\n",
    "\n",
    "# Plot the datasets\n",
    "plot_spirals_splits(train_dataset.data, test_dataset.data)\n",
    "\n",
    "# Plot the values over the input space\n",
    "plot_model_input_space(model, train_dataset, test_dataset, x_min=-30, x_max=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the model has failed to generalize to our target function $f$, and has simply just over-fit the distribution seen during training. \n",
    "\n",
    "However, our model learned this problem before, potentially we just need to re-train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss and optimizer\n",
    "model = MultiLayerPerceptron(\n",
    "    input_dim = 2,\n",
    "    output_dim = 1,\n",
    "    hidden_layers = 2,\n",
    "    hidden_dim = 8\n",
    ").to(device) #\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Let's run the training loop!\n",
    "num_epochs = 1000\n",
    "train_losses = []\n",
    "valid_losses = {}\n",
    "train_loss, valid_loss = .0, .0\n",
    "with tqdm(range(num_epochs)) as pbar:\n",
    "  for epoch in pbar:\n",
    "    # Training\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses, model = train_epoch(train_dataset, model, loss_fn, opt)\n",
    "    train_losses += _losses\n",
    "    train_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "    # Validation\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses = test_epoch(test_dataset, model, loss_fn)\n",
    "    valid_losses[epoch*len(train_dataset)] = _losses[0]\n",
    "    valid_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "# Plot the training performance\n",
    "plot_training_performance(train_losses, valid_losses)\n",
    "\n",
    "# Plot the values over the input space\n",
    "plot_model_input_space(model, train_dataset, test_dataset, x_min=-30, x_max=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this new spiral dataset we have significantly increased the size and complexity of the distribution $\\mathcal{H}_{D}$, our previous model hypothesis space $\\mathcal{H}_{\\theta}$ was \"well-balanced\" for the complexity of that distribution. However, with the new learning problem our models representational capacity seems to be ill-suited to this task. \n",
    "\n",
    "As with before, we can increase the size and complexity of our model hypothesis space $\\mathcal{H}_{\\theta}$ by increasing the model size to potentially counter-act this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss and optimizer\n",
    "model = MultiLayerPerceptron(\n",
    "    input_dim = 2,\n",
    "    output_dim = 1,\n",
    "    hidden_layers = 4,\n",
    "    hidden_dim = 8\n",
    ").to(device) #\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Let's run the training loop!\n",
    "num_epochs = 1000\n",
    "train_losses = []\n",
    "valid_losses = {}\n",
    "train_loss, valid_loss = .0, .0\n",
    "with tqdm(range(num_epochs)) as pbar:\n",
    "  for epoch in pbar:\n",
    "    # Training\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses, model = train_epoch(train_dataset, model, loss_fn, opt)\n",
    "    train_losses += _losses\n",
    "    train_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "    # Validation\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses = test_epoch(test_dataset, model, loss_fn)\n",
    "    valid_losses[epoch*len(train_dataset)] = _losses[0]\n",
    "    valid_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "# Plot the training performance\n",
    "plot_training_performance(train_losses, valid_losses)\n",
    "\n",
    "# Plot the values over the input space\n",
    "plot_model_input_space(model, train_dataset, test_dataset, x_min=-30, x_max=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we are lucky enough to be able to simply leveraging scaling laws to increase our model hypothesis space $\\mathcal{H}_{\\theta}$ to better capture this more complex data distribution. However this isn't always the case, sometimes certain architectures can only take us so far before we run into other limitations or issues inherent in their design... \n",
    "\n",
    "Understanding how the inductive biases of a model interacts with a given dataset and learning problem can provide rich insights into better ways to design models or modify the learning problem to better suit our desired outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to figure out a good balance between these two variables can take quite a bit of trial and error and is mostly a heuristic process based on experience and best practices. We can look at the evolution of NLP from Hidden Markov Models to LSTMS to Transformers and see this iterative development to design models to better capture and learn from larger and more complex datasets. Even just simply understanding where you sit in terms of under-fitting or over-fitting can be quite informative, there's a plethora of research into LLM scaling laws such as [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) (this paper formed the basis for OpenAI to explore scaling GPT-3 to 175B parameters) and [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556v1) are quite good in this regard to see that understanding this fundamental trade-off is an essential aspect in designing performant models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### [Section 3C] **Hacky Prior Knowledge**\n",
    "\n",
    "We also note that if we look at the models predictions outside the domain it's clear it's still just over-fitting the training distribution and not learning the underlying data generation process.\n",
    "\n",
    "Within the context of our toy dataset, no matter how we balance the previous factors, we will always have some approximation $\\epsilon_{app}$ and thus generalization $\\epsilon_{gen}$ error. For a model to learn our target function $f$ it need to be capable of representing the rules governing the data generating process $r(\\theta) = \\alpha + \\beta * \\theta + \\mathcal{N}(\\mu,\\sigma)$. Whilst as a universal function approximator an infinitely wide MLP would be able to capture this solution, however, in-reality with finite width and depth networks we can only learn almost anything.\n",
    "\n",
    "Generally in designing and applying machine learning, having an understanding and intuition of the underlying symmetries of a given problem can aid in incorporating different inductive priors into the model for a given task. Building in priors can act to constraint the hypothesis space $\\mathcal{H}$ allowing you to do more with less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the spiral data..\n",
    "data = generateSpiralDataset(\n",
    "    spiral_num_samples = 400, \n",
    "    spiral_noise = 0.5, \n",
    "    spiral_revolutions = 2.0\n",
    ")\n",
    "\n",
    "\"\"\" Say we had some intuition that the data has a relationship with the radius at a given point... \n",
    "\"\"\"\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "lbls = data[:,2]\n",
    "radius = torch.sqrt(x**2 + y**2)\n",
    "theta = torch.atan2(y, x)\n",
    "new_data = torch.stack([radius, theta, x, y, lbls], dim=1)\n",
    "\n",
    "# Create the datasets\n",
    "cartesian_train_dataset, cartesian_test_dataset = split_and_create_datasets(data, training_fraction=0.7)\n",
    "new_train_dataset, new_test_dataset = split_and_create_datasets(new_data, training_fraction=0.7)\n",
    "\n",
    "# Plot the datasets\n",
    "plot_spirals_splits(cartesian_train_dataset.data, cartesian_test_dataset.data)\n",
    "\n",
    "# Increase the size of the model until you see if you get slightly better performance...\n",
    "model = MultiLayerPerceptron(\n",
    "    input_dim = 4,\n",
    "    output_dim = 1,\n",
    "    hidden_layers = 4,\n",
    "    hidden_dim = 8\n",
    ").to(device) #\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Let's run the training loop!\n",
    "num_epochs = 2000\n",
    "train_losses = []\n",
    "valid_losses = {}\n",
    "train_loss, valid_loss = .0, .0\n",
    "with tqdm(range(num_epochs)) as pbar:\n",
    "  for epoch in pbar:\n",
    "    # Training\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses, model = train_epoch(new_train_dataset, model, loss_fn, opt)\n",
    "    train_losses += _losses\n",
    "    train_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "    # Validation\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses = test_epoch(new_test_dataset, model, loss_fn)\n",
    "    valid_losses[epoch*len(train_dataset)] = _losses[0]\n",
    "    valid_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "# Plot the training performance\n",
    "plot_training_performance(train_losses, valid_losses)\n",
    "\n",
    "\n",
    "def plot_model_input_space(model, train_dataset, test_dataset, x_min, x_max, x_res: Optional[int] = 120):\n",
    "    # Create grid points\n",
    "    X = np.linspace(x_min, x_max, x_res).astype(np.float32)\n",
    "    Y = np.linspace(x_min, x_max, x_res).astype(np.float32)\n",
    "\n",
    "    # Re-order the results \n",
    "    import math\n",
    "    bg = np.zeros((x_res, x_res))\n",
    "    for idx, x in enumerate(X):\n",
    "        for jdx, y in enumerate(Y):\n",
    "            # evaluate model at point\n",
    "            radius = math.sqrt(x**2 + y**2)\n",
    "            theta = math.atan2(y, x)\n",
    "            point = torch.tensor([[radius, theta, x, y]]) # NOTE: Add in the new input features\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_p = model(point.to(device)).cpu()\n",
    "            \n",
    "            # store result at point\n",
    "            bg[jdx, idx] = y_p.item()\n",
    "\n",
    "    # Clip values\n",
    "    bg[bg < 0] = .0\n",
    "    bg[bg > 1] = 1.\n",
    "\n",
    "    # Define normalization bounds \n",
    "    from matplotlib.colors import Normalize\n",
    "    norm = Normalize(vmin=0, vmax=1)\n",
    "\n",
    "    # Lets create a custom colorm\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    colors = [\"tab:orange\", \"white\", \"tab:blue\"]\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n",
    "\n",
    "    # Map the values to the colour map\n",
    "    bg_colors = cmap(bg)[::-1,::-1]\n",
    "\n",
    "    # Plot the results\n",
    "    from matplotlib.cm import ScalarMappable\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(bg_colors, extent=[x_min, x_max, x_min, x_max], origin=\"lower\", aspect=\"auto\", alpha=0.25)\n",
    "    fig.colorbar(ScalarMappable(norm=norm, cmap=cmap), ax=ax, label='Value')\n",
    "\n",
    "    # Lets also overlay the training and testing data\n",
    "    ax.scatter(train_dataset.data[train_dataset.data[:,2] == 0,0], train_dataset.data[train_dataset.data[:,2] == 0,1], alpha=0.15, color=\"tab:blue\", label=\"Spiral A: Train\")\n",
    "    ax.scatter(train_dataset.data[train_dataset.data[:,2] == 1,0], train_dataset.data[train_dataset.data[:,2] == 1,1], alpha=0.15, color=\"tab:orange\", label=\"Spiral B: Train\")\n",
    "    ax.scatter(test_dataset.data[test_dataset.data[:,2] == 0,0], test_dataset.data[test_dataset.data[:,2] == 0,1], color=\"tab:blue\", label=\"Spiral A: Test\")\n",
    "    ax.scatter(test_dataset.data[test_dataset.data[:,2] == 1,0], test_dataset.data[test_dataset.data[:,2] == 1,1], color=\"tab:orange\", label=\"Spiral B: Test\")\n",
    "\n",
    "    # Other figure stuff\n",
    "    ax.set_title(\"Network Outputs\")\n",
    "    ax.set_xlim(left=x_min, right=x_max)\n",
    "    ax.set_ylim(bottom=x_min, top=x_max)\n",
    "    ax.set_xlabel(\"x_1\")\n",
    "    ax.set_ylabel(\"x_2\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.grid(True, alpha=0.25)\n",
    "\n",
    "\n",
    "# Plot the values over the input space\n",
    "plot_model_input_space(model, cartesian_train_dataset, cartesian_test_dataset, x_min=-30, x_max=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that we didn't do anything particularly sophisticated or even what would generally be considered good practice, but through the \"magic\" of deep-learning it paid off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Section 4] Building an Informed Model\n",
    "\n",
    "Here's another example where considering the data can inform the model architecture, here we have a dataset which generates data following the rule $f(x_{1}, x_{2}) = x_{1} + x_{2}$.\n",
    "\n",
    "See if you can design a model which can learn to represent the underlying data generating process perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset class\n",
    "class SumDataset(torch.utils.data.Dataset):\n",
    "  \"\"\" Dataset\n",
    "\n",
    "  Simple dataset which generates data following the rule: \n",
    "    y = x_1 + x_2\n",
    "\n",
    "  Where: \n",
    "    x_1 in [low, high] e.g. [0,10]\n",
    "    x_2 in [low, high] e.g. [0,10]\n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self, low: int, high: int, batch_size: int):\n",
    "    self.low = low\n",
    "    self.high = high\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  def __iter__(self):\n",
    "    while True:\n",
    "      yield self.get_batch()\n",
    "\n",
    "  def get_batch(self):\n",
    "    # define some random numbers between [low,high]\n",
    "    x_1 = torch.randint(low=self.low, high=self.high, size=(self.batch_size,1), dtype=torch.float32)\n",
    "    x_2 = torch.randint(low=self.low, high=self.high, size=(self.batch_size,1), dtype=torch.float32)\n",
    "    x = torch.cat((x_1, x_2), dim=1)\n",
    "\n",
    "    # define the target value following our rule\n",
    "    y = x_1 + x_2\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a MLP class\n",
    "class YourModel(nn.Module):\n",
    "  \"\"\" Multi-layer perceptron\n",
    "  \"\"\"\n",
    "  def __init__(self, ...):\n",
    "    super(YourModel, self).__init__()\n",
    "\n",
    "    # Create model layers\n",
    "    modules = []\n",
    "    modules.append(...)\n",
    "    \n",
    "    # Create the model\n",
    "    self.model = nn.Sequential(*modules)\n",
    "\n",
    "  def forward(self, x):\n",
    "    y_p = self.model(x)\n",
    "    return y_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# model\n",
    "model = YourModel(...).to(device)\n",
    "\n",
    "# loss function\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# dataset\n",
    "dataset_train = SumDataset(low=0, high=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# outputs\n",
    "losses = []\n",
    "\n",
    "#\n",
    "n_epochs = 20000\n",
    "\n",
    "# training loop\n",
    "with tqdm(dataset_train, total=n_epochs) as pbar:\n",
    "  for idx, (x, y) in enumerate(pbar):\n",
    "    # move data to device\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # forwards pass\n",
    "    y_p = model(x)\n",
    "\n",
    "    # compute the loss\n",
    "    l = loss(y_p, y)\n",
    "\n",
    "    # performance the backwards pass (compute gradients)\n",
    "    l.backward()\n",
    "\n",
    "    # perform step of the optimizer and clear gradients\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # update description\n",
    "    pbar.set_description(f\"[TRAIN] epoch: {idx}/{n_epochs} | loss: {l.item():.4f}\")\n",
    "    t_losses.append(l.item())\n",
    "\n",
    "    # stopping criterion\n",
    "    if idx >= n_epochs: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot training losses\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.semilogy(t_losses, linestyle=\"-\", color=\"tab:blue\", label=\"training\", alpha=0.75)\n",
    "ax.set_xlim(left=0, right=len(t_losses))\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Training Loss\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.grid(True, alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate the model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.data.shape} / {param.data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
