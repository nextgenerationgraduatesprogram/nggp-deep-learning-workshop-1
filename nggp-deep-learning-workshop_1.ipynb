{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NextGen2024**: Deep Learning Workshop 1 \n",
    "\n",
    "Within these workshops, we will dive into the learning problem, and explore what control we have as practitioners. Understand the balance and trade-offs between different variables can provide rich insight into the design and function of modern deep learning systems and directions.\n",
    "\n",
    "We will explore this within the context of some simple models and toy datasets before moving onto explore these within the context of more modern recent architectures during our next workshop.\n",
    "\n",
    "Through these workshops I hope to convey a better understanding of the fundamental trade-offs in the design of deep learning systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from typing import *\n",
    "from torch import Tensor\n",
    "\n",
    "# seed training (deterministic results)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preamble] The Learning Problem\n",
    "\n",
    "**The Hypothesis Space**: Lets refer to the set of possible models obtained from the training process as the hypothesis space $\\mathcal{H}$. We'll also denote the set of possible solutions as the solution space $\\mathcal{S}$.\n",
    "\n",
    "<img src=\"Slide1.JPG\" alt=\"Slide 1\" width=\"600\"/>\n",
    "\n",
    "\n",
    "In machine learning, the hypothesis space $\\mathcal{H}$ is typically determine by of our choice of model $\\mathcal{H}_{\\theta}$, data $\\mathcal{H}_{D}$, and optimization $\\mathcal{H}_{o}$ spaces.\n",
    "\n",
    "<img src=\"Slide2.JPG\" alt=\"Slide 2\" width=\"600\"/>\n",
    "\n",
    "\n",
    "**Approximation Error**: The approximation error $\\epsilon_{app}$ is the error between our target function $f$ the best possible model $h^{*} \\in \\mathcal{H}$. This error represents the *BEST* performance we can get out of our hypothesis space.\n",
    "\n",
    "**Generalization Error**: The generalization error $\\epsilon_{gen}$ is the error between our target function $f$ and our trained model $h \\in \\mathcal{H}$. This error represents the *ACTUAL* performance obtained as a result of the learning process.\n",
    "\n",
    "**Estimation Error**: The estimation error $\\epsilon_{est}$ is the difference in error between our trained model $h$ and the optimal model $h^{*}$.\n",
    "\n",
    "<img src=\"Slide3.JPG\" alt=\"Slide 3\" width=\"600\"/>\n",
    "\n",
    "Within these workshops we will explore how the **model** and **dataset** interact and affect the design and outcome of the learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## [Section 1] Spiral Classification\n",
    "\n",
    "Let's say we have collected some data from some physical system that produces a set of spirals, and we're interested in classifying the different spiral arms generated.\n",
    "\n",
    "<img src=\"TwoSpirals.jpg\" alt=\"Spiral\" width=\"300\"/>\n",
    "\n",
    "Thankfully it so happens that the data generated follows a fairly simple target function $r(\\theta) = \\alpha + \\beta * \\theta + \\mathcal{N}(\\mu,\\sigma)$, in reality with deep-learning related tasks we rarely know or can even represent this target function.\n",
    "\n",
    "Let us see what data we've managed to collect first..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data**\n",
    "\n",
    "We'll generate some data following this rule, and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets define our data generating process!\n",
    "\"\"\"\n",
    "def generateSpiralDataset(\n",
    "        spiral_num_samples: int,\n",
    "        spiral_noise: Optional[float] = 1., \n",
    "        spiral_revolutions: Optional[float] = 1,\n",
    "    ):\n",
    "        \"\"\" Lets use two simple archimedes spirals going in opposite directions using the\n",
    "        following rule: radius = alpha * theta\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [N * 3] where the columns contain [x-value, y-value, label] \n",
    "\n",
    "        \"\"\"\n",
    "        # Theta \n",
    "        theta = 2 * np.pi * spiral_revolutions * np.sqrt(np.random.rand(spiral_num_samples))\n",
    "\n",
    "        # Spiral A\n",
    "        radius_A = 2 * theta + np.pi / 2 # radius increases linearly with theta - anti-clockwise\n",
    "        x_A = radius_A * np.cos(theta) # x-component\n",
    "        y_A = radius_A * np.sin(theta) # y-component\n",
    "        spiral_A_data = np.array([x_A, y_A]).T # pack them together: column 0 = x-values, column 1 = y-values\n",
    "        spiral_A_data += spiral_noise * np.random.randn(spiral_num_samples,2) # add some noise\n",
    "        spiral_A_lbls = np.zeros((spiral_num_samples,1)) # spiral A = 0\n",
    "        spiral_A = np.append(spiral_A_data, spiral_A_lbls, axis=1)\n",
    "            \n",
    "        # Spiral B\n",
    "        radius_B = -2 * theta - np.pi / 2 # clockwise\n",
    "        x_B = radius_B * np.cos(theta)\n",
    "        y_B = radius_B * np.sin(theta)\n",
    "        spiral_B_data = np.array([x_B, y_B]).T\n",
    "        spiral_B_data += spiral_noise * np.random.randn(spiral_num_samples,2) \n",
    "        spiral_B_lbls = np.ones((spiral_num_samples,1)) # spiral B = 1\n",
    "        spiral_B = np.append(spiral_B_data, spiral_B_lbls, axis=1)\n",
    "\n",
    "        # Put the data into one big array\n",
    "        spirals = np.append(spiral_A, spiral_B, axis=0)\n",
    "        spirals = torch.from_numpy(spirals).to(dtype=torch.float32)\n",
    "\n",
    "        return spirals\n",
    "\n",
    "# Generate the spiral dataset\n",
    "data = generateSpiralDataset(\n",
    "    spiral_num_samples = 100, \n",
    "    spiral_noise = 0.5, \n",
    "    spiral_revolutions = 1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets plot the dataset, if possible using a color for each spiral...\n",
    "\"\"\" \n",
    "# Begin by separating the dataset into the different spirals for plotting...\n",
    "spiral_A_x = data[data[:,2] == 0][:,0]\n",
    "spiral_A_y = data[data[:,2] == 0][:,1]\n",
    "spiral_B_x = data[data[:,2] == 1][:,0]\n",
    "spiral_B_y = data[data[:,2] == 1][:,1]\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(spiral_A_x, spiral_A_y, alpha=0.75, label=\"Spiral A\")\n",
    "ax.scatter(spiral_B_x, spiral_B_y, alpha=0.75, label=\"Spiral B\")\n",
    "ax.set_title(\"Spiral Dataset\")\n",
    "ax.grid(True, alpha=0.25)\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset**\n",
    "\n",
    "Looks good!\n",
    "\n",
    "We decide to split up our dataset into training and testing sets, and create a dataset to draw samples from during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Split up our data into training and testing!\n",
    "\"\"\"\n",
    "class SpiralDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data: Tensor, batch_size: Optional[int] = None):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size if batch_size is not None else self.data.shape[0]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.data.shape[0] // self.batch_size\n",
    "\n",
    "    def __iter__(self) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\" Yield data from the dataset in batches.\n",
    "        \"\"\" \n",
    "        n_samples = self.__len__()\n",
    "        for idx in range(n_samples):\n",
    "            # Extract data for current batch \n",
    "            sample = self.data[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "\n",
    "            # Define inputs: x/y-values\n",
    "            x = sample[:,0:-1]\n",
    "\n",
    "            # Define targets: labels\n",
    "            y = sample[:,-1].unsqueeze(-1)\n",
    "\n",
    "            yield x, y\n",
    "\n",
    "\n",
    "# Create a function for splitting the data and creating the datasets\n",
    "def split_and_create_datasets(data: Tensor, training_fraction: Optional[float] = 0.7) -> Tuple[SpiralDataset, SpiralDataset]:\n",
    "    # Define the number of samples to use during training vs. testing\n",
    "    n_train = int(training_fraction * data.shape[0])\n",
    "    n_test = data.shape[0] - n_train\n",
    "\n",
    "    # Split the dataset into training/testing set\n",
    "    train_idxs, valid_idxs = tuple([s.indices for s in random_split(range(data.shape[0]), [n_train, n_test])])\n",
    "    train_data = data[train_idxs]\n",
    "    test_data = data[valid_idxs]\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = SpiralDataset(train_data)\n",
    "    test_dataset = SpiralDataset(test_data)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "# Split data and create datasets\n",
    "train_dataset, test_dataset = split_and_create_datasets(data, training_fraction=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets plot the results\n",
    "\"\"\"\n",
    "# Create a function for plotting the spirals\n",
    "def plot_spirals_splits(train_data, test_data):\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    ax.scatter(train_data[train_data[:,2] == 0,0], train_data[train_data[:,2] == 0,1], alpha=0.15, color=\"tab:blue\", label=\"Spiral A: Train\")\n",
    "    ax.scatter(train_data[train_data[:,2] == 1,0], train_data[train_data[:,2] == 1,1], alpha=0.15, color=\"tab:orange\", label=\"Spiral B: Train\")\n",
    "    ax.scatter(test_data[test_data[:,2] == 0,0], test_data[test_data[:,2] == 0,1], color=\"tab:blue\", label=\"Spiral A: Test\")\n",
    "    ax.scatter(test_data[test_data[:,2] == 1,0], test_data[test_data[:,2] == 1,1], color=\"tab:orange\", label=\"Spiral B: Test\")\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    ax.set_title(f\"Spiral Dataset\")\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "# Plot the spirals\n",
    "plot_spirals_splits(train_dataset.data, test_dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model**\n",
    "\n",
    "Being freshly minted ML practitioners we decide to take a stab at modelling this dataset using a multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets define the architecture of our model!\n",
    "\"\"\"\n",
    "class Perceptron(nn.Module):\n",
    "    \"\"\" Create a single-layer perceptron with ReLU activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        input_dim: int, \n",
    "        output_dim: int\n",
    "    ):\n",
    "        super(Perceptron, self).__init__()\n",
    "\n",
    "        # Create linear layer\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "        # Create non-linear activation function\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    \"\"\" Create a multi-layer perceptron by stacking multiple layers of perceptrons.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "        input_dim: Optional[int] = 2, \n",
    "        output_dim: Optional[int] = 1, \n",
    "        hidden_layers: Optional[int] = 0, \n",
    "        hidden_dim: Optional[int] = 4\n",
    "    ):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        # Create input layer\n",
    "        modules = [Perceptron(input_dim, hidden_dim)]\n",
    "\n",
    "        # Create hidden layers\n",
    "        for _ in range(hidden_layers):\n",
    "            modules.append(Perceptron(hidden_dim, hidden_dim))\n",
    "\n",
    "        # Create output layer\n",
    "        modules.append(Perceptron(hidden_dim, output_dim))\n",
    "\n",
    "        # Create the model\n",
    "        self.layers = nn.ModuleList(modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training**\n",
    "\n",
    "We have our **dataset** and **model**, lets train up our spiral classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Next we're going to create the model, optimizer, loss function we want to train!\n",
    "\"\"\"\n",
    "# device\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# model\n",
    "model = MultiLayerPerceptron(input_dim=2, output_dim=1, hidden_layers=1, hidden_dim=4).to(device)\n",
    "\n",
    "# loss function\n",
    "loss_fn = torch.nn.MSELoss() # there are better loss functions but we'll use this for now\n",
    "\n",
    "# optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets train our model!\n",
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_epoch(dataset: Callable, model: Callable, loss_fn: Callable, opt: Callable):\n",
    "  \"\"\" Performs a training epoch.\n",
    "  \"\"\"\n",
    "  losses = []\n",
    "  for idx, (x, y) in enumerate(dataset):\n",
    "    # move data to device\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # forwards pass\n",
    "    y_p = model(x)\n",
    "\n",
    "    # compute the loss\n",
    "    l = loss_fn(y_p, y)\n",
    "\n",
    "    # performance the backwards pass (compute gradients)\n",
    "    l.backward()\n",
    "\n",
    "    # perform step of the optimizer and clear gradients\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # append loss\n",
    "    losses.append(l.item())\n",
    "\n",
    "  return losses, model\n",
    "\n",
    "\n",
    "def test_epoch(dataset: Callable, model: Callable, loss_fn: Callable):\n",
    "  \"\"\" Performs a testing epoch.\n",
    "  \"\"\"\n",
    "  losses = []\n",
    "  with torch.no_grad():\n",
    "    for idx, (x, y) in enumerate(dataset):\n",
    "      # move data to device\n",
    "      x = x.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # forwards pass\n",
    "      y_p = model(x)\n",
    "\n",
    "      # compute the loss\n",
    "      l = loss_fn(y_p, y)\n",
    "\n",
    "      # append loss\n",
    "      losses.append(l.item())\n",
    "    \n",
    "  return losses\n",
    "\n",
    "\n",
    "# Let's run the training loop!\n",
    "num_epochs = 1000\n",
    "train_losses = []\n",
    "valid_losses = {}\n",
    "train_loss, valid_loss = .0, .0\n",
    "with tqdm(range(num_epochs)) as pbar:\n",
    "  for epoch in pbar:\n",
    "    # Training\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses, model = train_epoch(train_dataset, model, loss_fn, opt)\n",
    "    train_losses += _losses\n",
    "    train_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "    # Validation\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses = test_epoch(test_dataset, model, loss_fn)\n",
    "    valid_losses[epoch*len(train_dataset)] = _losses[0]\n",
    "    valid_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets plot the training performance using the losses\n",
    "\"\"\"\n",
    "# Plot losses from training\n",
    "def plot_training_performance(train_losses, valid_losses):\n",
    "  fig = plt.figure(figsize=(16,4))\n",
    "  ax = fig.add_subplot(111)\n",
    "  ax.plot(train_losses, linestyle=\"-\", color=\"tab:blue\", label=\"Train\", alpha=0.75)\n",
    "  ax.plot(valid_losses.keys(), valid_losses.values(), linestyle=\"-\", color=\"tab:orange\", label=\"Valid\", alpha=0.75)\n",
    "  ax.set_xlim(left=0, right=len(train_losses))\n",
    "  ax.set_ylim(bottom=0, top=1)\n",
    "  ax.set_xlabel(\"Iterations\")\n",
    "  ax.set_ylabel(\"Loss\")\n",
    "  ax.set_title(\"Training\")\n",
    "  ax.legend(loc=\"best\")\n",
    "  ax.grid(True, alpha=0.25)\n",
    "\n",
    "\n",
    "# Plot \n",
    "plot_training_performance(train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Lets also check out how the model is making decisions over the input space.\n",
    "\"\"\"\n",
    "def plot_model_input_space(model, train_dataset, test_dataset, x_min, x_max, x_res: Optional[int] = 120):\n",
    "    # Create grid points\n",
    "    X1 = np.linspace(x_min, x_max, x_res).astype(np.float32)\n",
    "    X2 = np.linspace(x_min, x_max, x_res).astype(np.float32)\n",
    "\n",
    "    # Re-order the results \n",
    "    bg = np.zeros((x_res, x_res))\n",
    "    for idx, x1 in enumerate(X1):\n",
    "        for jdx, x2 in enumerate(X2):\n",
    "            # evaluate model at point\n",
    "            point = torch.tensor([[x1, x2]])\n",
    "            with torch.no_grad():\n",
    "                y_p = model(point.to(device)).cpu()\n",
    "            \n",
    "            # store result at point\n",
    "            bg[jdx, idx] = y_p.item()\n",
    "\n",
    "    # Clip values\n",
    "    bg[bg < 0] = .0\n",
    "    bg[bg > 1] = 1.\n",
    "\n",
    "    # Define normalization bounds \n",
    "    from matplotlib.colors import Normalize\n",
    "    norm = Normalize(vmin=0, vmax=1)\n",
    "\n",
    "    # Lets create a custom colorm\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    colors = [\"tab:orange\", \"white\", \"tab:blue\"]\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n",
    "\n",
    "    # Map the values to the colour map\n",
    "    bg_colors = cmap(bg)[::-1,::-1]\n",
    "\n",
    "    # Plot the results\n",
    "    from matplotlib.cm import ScalarMappable\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(bg_colors, extent=[x_min, x_max, x_min, x_max], origin=\"lower\", aspect=\"auto\", alpha=0.25)\n",
    "    fig.colorbar(ScalarMappable(norm=norm, cmap=cmap), ax=ax, label='Value')\n",
    "\n",
    "    # Lets also overlay the training and testing data\n",
    "    ax.scatter(train_dataset.data[train_dataset.data[:,2] == 0,0], train_dataset.data[train_dataset.data[:,2] == 0,1], alpha=0.15, color=\"tab:blue\", label=\"Spiral A: Train\")\n",
    "    ax.scatter(train_dataset.data[train_dataset.data[:,2] == 1,0], train_dataset.data[train_dataset.data[:,2] == 1,1], alpha=0.15, color=\"tab:orange\", label=\"Spiral B: Train\")\n",
    "    ax.scatter(test_dataset.data[test_dataset.data[:,2] == 0,0], test_dataset.data[test_dataset.data[:,2] == 0,1], color=\"tab:blue\", label=\"Spiral A: Test\")\n",
    "    ax.scatter(test_dataset.data[test_dataset.data[:,2] == 1,0], test_dataset.data[test_dataset.data[:,2] == 1,1], color=\"tab:orange\", label=\"Spiral B: Test\")\n",
    "\n",
    "    # Other figure stuff\n",
    "    ax.set_title(\"Network Outputs\")\n",
    "    ax.set_xlim(left=x_min, right=x_max)\n",
    "    ax.set_ylim(bottom=x_min, top=x_max)\n",
    "    ax.set_xlabel(\"x_1\")\n",
    "    ax.set_ylabel(\"x_2\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.grid(True, alpha=0.25)\n",
    "\n",
    "\n",
    "# Plot the values over the input space\n",
    "plot_model_input_space(model, train_dataset, test_dataset, x_min=-15, x_max=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Model Bias-Complexity Trade-off**\n",
    "\n",
    "As we increase the size and complexity of our hypothesis class $\\mathcal{H}$ (i.e. creating a larger and higher capacity model), the approximation error $\\epsilon_{app}$ generally *decreases* as our best possible model becomes more capable of fitting the target function $f$. \n",
    "\n",
    "However, the estimation error $\\epsilon_{est}$ **may** also *increase* as the model becomes more capable of over-fitting the data distribution seen during training and not learning the lower dimensional rules governing the data generation process.\n",
    "\n",
    "<img src=\"Slide5.JPG\" alt=\"Slide 5\" width=\"600\"/>\n",
    "<img src=\"Slide6.JPG\" alt=\"Slide 6\" width=\"600\"/>\n",
    "\n",
    "Our learning problem has a fixed complexity, we need to find the balance between under-fitting and over-fitting the training distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss and optimizer\n",
    "model = MultiLayerPerceptron(\n",
    "  input_dim = 2,\n",
    "  output_dim = 1,\n",
    "  hidden_layers = 2,\n",
    "  hidden_dim = 8\n",
    ").to(device)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Let's run the training loop!\n",
    "num_epochs = 1000\n",
    "train_losses = []\n",
    "valid_losses = {}\n",
    "train_loss, valid_loss = .0, .0\n",
    "with tqdm(range(num_epochs)) as pbar:\n",
    "  for epoch in pbar:\n",
    "    # Training\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses, model = train_epoch(train_dataset, model, loss_fn, opt)\n",
    "    train_losses += _losses\n",
    "    train_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "    # Validation\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses = test_epoch(test_dataset, model, loss_fn)\n",
    "    valid_losses[epoch*len(train_dataset)] = _losses[0]\n",
    "    valid_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "# Plot the training performance\n",
    "plot_training_performance(train_losses, valid_losses)\n",
    "\n",
    "# Plot the values over the input space\n",
    "plot_model_input_space(model, train_dataset, test_dataset, x_min=-15, x_max=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Dataset Bias-Complexity Trade-off**\n",
    "\n",
    "However, we also have to consider the other aspects of the learning problem, the complexity of the distribution also plays a role in defining the hypothesis class $\\mathcal{H}$. As we increase the size and complexity of the distribution space $\\mathcal{H}_{D}$ we generally expect our approximation error $\\epsilon_{app}$ to increase for a fixed model. The flip-side of this is also generally true.\n",
    "\n",
    "Peferably, we would collect a distribution that is fully representative of the rules which govern the data generation process, however this is generally not feasible. We typically like to make sure we have a *good enough* representation with respect to the distribution we expect to see during deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we decide to apply our model to another similar problem! Hey this binary star system has spirals, let's assume the spirals are the same as before but just extend for longer.\n",
    "\n",
    "<img src=\"CelestialSpiral.jpg\" alt=\"Spiral\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the spiral data : but sparser...\n",
    "data = generateSpiralDataset(\n",
    "    spiral_num_samples = 400, \n",
    "    spiral_noise = 0.5, \n",
    "    spiral_revolutions = 2.0\n",
    ")\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset, test_dataset = split_and_create_datasets(data, training_fraction=0.7)\n",
    "\n",
    "# Plot the datasets\n",
    "plot_spirals_splits(train_dataset.data, test_dataset.data)\n",
    "\n",
    "# Plot the values over the input space\n",
    "plot_model_input_space(model, train_dataset, test_dataset, x_min=-30, x_max=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the model has failed to generalize to our target function $f$, and has simply just over-fit the distribution seen during training. \n",
    "\n",
    "However, our model learned this problem before, potentially we just need to re-train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss and optimizer\n",
    "model = MultiLayerPerceptron(\n",
    "    input_dim = 2,\n",
    "    output_dim = 1,\n",
    "    hidden_layers = 2,\n",
    "    hidden_dim = 8\n",
    ").to(device) #\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Let's run the training loop!\n",
    "num_epochs = 1000\n",
    "train_losses = []\n",
    "valid_losses = {}\n",
    "train_loss, valid_loss = .0, .0\n",
    "with tqdm(range(num_epochs)) as pbar:\n",
    "  for epoch in pbar:\n",
    "    # Training\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses, model = train_epoch(train_dataset, model, loss_fn, opt)\n",
    "    train_losses += _losses\n",
    "    train_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "    # Validation\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses = test_epoch(test_dataset, model, loss_fn)\n",
    "    valid_losses[epoch*len(train_dataset)] = _losses[0]\n",
    "    valid_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "# Plot the training performance\n",
    "plot_training_performance(train_losses, valid_losses)\n",
    "\n",
    "# Plot the values over the input space\n",
    "plot_model_input_space(model, train_dataset, test_dataset, x_min=-30, x_max=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this new spiral dataset we have significantly increased the size and complexity of the distribution $\\mathcal{H}_{D}$, our previous model hypothesis space $\\mathcal{H}_{\\theta}$ was \"well-balanced\" for the complexity of that distribution. However, with the new learning problem our models representational capacity seems to be ill-suited to this task. \n",
    "\n",
    "As with before, we can increase the size and complexity of our model hypothesis space $\\mathcal{H}_{\\theta}$ by increasing the model size to potentially counter-act this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss and optimizer\n",
    "model = MultiLayerPerceptron(\n",
    "    input_dim = 2,\n",
    "    output_dim = 1,\n",
    "    hidden_layers = 4,\n",
    "    hidden_dim = 8\n",
    ").to(device) #\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Let's run the training loop!\n",
    "num_epochs = 1000\n",
    "train_losses = []\n",
    "valid_losses = {}\n",
    "train_loss, valid_loss = .0, .0\n",
    "with tqdm(range(num_epochs)) as pbar:\n",
    "  for epoch in pbar:\n",
    "    # Training\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses, model = train_epoch(train_dataset, model, loss_fn, opt)\n",
    "    train_losses += _losses\n",
    "    train_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "    # Validation\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses = test_epoch(test_dataset, model, loss_fn)\n",
    "    valid_losses[epoch*len(train_dataset)] = _losses[0]\n",
    "    valid_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "# Plot the training performance\n",
    "plot_training_performance(train_losses, valid_losses)\n",
    "\n",
    "# Plot the values over the input space\n",
    "plot_model_input_space(model, train_dataset, test_dataset, x_min=-30, x_max=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we are lucky enough to be able to simply leveraging scaling laws to increase our model hypothesis space $\\mathcal{H}_{\\theta}$ to better capture this more complex data distribution. However this isn't always the case, sometimes certain architectures can only take us so far before we run into other limitations or issues inherent in their design... \n",
    "\n",
    "Understanding how the inductive biases of a model interacts with a given dataset and learning problem can provide rich insights into better ways to design models or modify the learning problem to better suit our desired outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to figure out a good balance between these two variables can take quite a bit of trial and error and is mostly a heuristic process based on experience and best practices. We can look at the evolution of NLP from Hidden Markov Models to LSTMS to Transformers and see this iterative development to design models to better capture and learn from larger and more complex datasets. Even just simply understanding where you sit in terms of under-fitting or over-fitting can be quite informative, there's a plethora of research into LLM scaling laws such as [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) (this paper formed the basis for OpenAI to explore scaling GPT-3 to 175B parameters) and [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556v1) are quite good in this regard to see that understanding this fundamental trade-off is an essential aspect in designing performant models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Incorporating Priors**\n",
    "\n",
    "We also note that if we look at the models predictions outside the domain it's clear it's still just over-fitting the training distribution and not learning the underlying data generation process.\n",
    "\n",
    "Within the context of our toy dataset, no matter how we balance the previous factors, we will always have some approximation $\\epsilon_{app}$ and thus generalization $\\epsilon_{gen}$ error. For a model to learn our target function $f$ it need to be capable of representing the rules governing the data generating process $r(\\theta) = \\alpha + \\beta * \\theta + \\mathcal{N}(\\mu,\\sigma)$. We can represent such as hypothesis space $\\mathcal{H}$ as encompassing the solution space $\\mathcal{S}$ and consequently $f$.\n",
    "\n",
    "Whilst as a universal function approximator our MLP *might* be able to capture this solution, but if we look at the complexity of the model in relation to the data generation process, it would seem far more likely for it to over-fit the solution.\n",
    "\n",
    "<img src=\"Slide10.JPG\" alt=\"Slide 10\" width=\"600\"/>\n",
    "\n",
    "Generally in designing and applying machine learning, having an understanding and intuition of the underlying symmetries of a given problem can aid in incorporating different inductive priors into the model for a given task. Building in priors can act to constraint the hypothesis space $\\mathcal{H}$ allowing you to do more with less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the spiral data..\n",
    "data = generateSpiralDataset(\n",
    "    spiral_num_samples = 400, \n",
    "    spiral_noise = 0.5, \n",
    "    spiral_revolutions = 2.0\n",
    ")\n",
    "\n",
    "\"\"\" Say we had some intuition that the data has a relationship with the radius at a given point... \n",
    "\"\"\"\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "lbls = data[:,2]\n",
    "radius = torch.sqrt(x**2 + y**2)\n",
    "theta = torch.atan2(y, x)\n",
    "new_data = torch.stack([radius, theta, x, y, lbls], dim=1)\n",
    "\n",
    "# Create the datasets\n",
    "cartesian_train_dataset, cartesian_test_dataset = split_and_create_datasets(data, training_fraction=0.7)\n",
    "new_train_dataset, new_test_dataset = split_and_create_datasets(new_data, training_fraction=0.7)\n",
    "\n",
    "# Plot the datasets\n",
    "plot_spirals_splits(cartesian_train_dataset.data, cartesian_test_dataset.data)\n",
    "\n",
    "# Increase the size of the model until you see if you get slightly better performance...\n",
    "model = MultiLayerPerceptron(\n",
    "    input_dim = 4,\n",
    "    output_dim = 1,\n",
    "    hidden_layers = 4,\n",
    "    hidden_dim = 8\n",
    ").to(device) #\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Let's run the training loop!\n",
    "num_epochs = 2000\n",
    "train_losses = []\n",
    "valid_losses = {}\n",
    "train_loss, valid_loss = .0, .0\n",
    "with tqdm(range(num_epochs)) as pbar:\n",
    "  for epoch in pbar:\n",
    "    # Training\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses, model = train_epoch(new_train_dataset, model, loss_fn, opt)\n",
    "    train_losses += _losses\n",
    "    train_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[TRAIN] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "    # Validation\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "    _losses = test_epoch(new_test_dataset, model, loss_fn)\n",
    "    valid_losses[epoch*len(train_dataset)] = _losses[0]\n",
    "    valid_loss = sum(_losses) / len(_losses)\n",
    "    pbar.set_description(f\"[VALID] Epoch: {epoch}/{num_epochs} | train loss: {train_loss:.3f} | valid loss: {valid_loss:.3f}\")\n",
    "\n",
    "# Plot the training performance\n",
    "plot_training_performance(train_losses, valid_losses)\n",
    "\n",
    "\n",
    "def plot_model_input_space(model, train_dataset, test_dataset, x_min, x_max, x_res: Optional[int] = 120):\n",
    "    # Create grid points\n",
    "    X = np.linspace(x_min, x_max, x_res).astype(np.float32)\n",
    "    Y = np.linspace(x_min, x_max, x_res).astype(np.float32)\n",
    "\n",
    "    # Re-order the results \n",
    "    import math\n",
    "    bg = np.zeros((x_res, x_res))\n",
    "    for idx, x in enumerate(X):\n",
    "        for jdx, y in enumerate(Y):\n",
    "            # evaluate model at point\n",
    "            radius = math.sqrt(x**2 + y**2)\n",
    "            theta = math.atan2(y, x)\n",
    "            point = torch.tensor([[radius, theta, x, y]]) # NOTE: Add in the new input features\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_p = model(point.to(device)).cpu()\n",
    "            \n",
    "            # store result at point\n",
    "            bg[jdx, idx] = y_p.item()\n",
    "\n",
    "    # Clip values\n",
    "    bg[bg < 0] = .0\n",
    "    bg[bg > 1] = 1.\n",
    "\n",
    "    # Define normalization bounds \n",
    "    from matplotlib.colors import Normalize\n",
    "    norm = Normalize(vmin=0, vmax=1)\n",
    "\n",
    "    # Lets create a custom colorm\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    colors = [\"tab:orange\", \"white\", \"tab:blue\"]\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n",
    "\n",
    "    # Map the values to the colour map\n",
    "    bg_colors = cmap(bg)[::-1,::-1]\n",
    "\n",
    "    # Plot the results\n",
    "    from matplotlib.cm import ScalarMappable\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(bg_colors, extent=[x_min, x_max, x_min, x_max], origin=\"lower\", aspect=\"auto\", alpha=0.25)\n",
    "    fig.colorbar(ScalarMappable(norm=norm, cmap=cmap), ax=ax, label='Value')\n",
    "\n",
    "    # Lets also overlay the training and testing data\n",
    "    ax.scatter(train_dataset.data[train_dataset.data[:,2] == 0,0], train_dataset.data[train_dataset.data[:,2] == 0,1], alpha=0.15, color=\"tab:blue\", label=\"Spiral A: Train\")\n",
    "    ax.scatter(train_dataset.data[train_dataset.data[:,2] == 1,0], train_dataset.data[train_dataset.data[:,2] == 1,1], alpha=0.15, color=\"tab:orange\", label=\"Spiral B: Train\")\n",
    "    ax.scatter(test_dataset.data[test_dataset.data[:,2] == 0,0], test_dataset.data[test_dataset.data[:,2] == 0,1], color=\"tab:blue\", label=\"Spiral A: Test\")\n",
    "    ax.scatter(test_dataset.data[test_dataset.data[:,2] == 1,0], test_dataset.data[test_dataset.data[:,2] == 1,1], color=\"tab:orange\", label=\"Spiral B: Test\")\n",
    "\n",
    "    # Other figure stuff\n",
    "    ax.set_title(\"Network Outputs\")\n",
    "    ax.set_xlim(left=x_min, right=x_max)\n",
    "    ax.set_ylim(bottom=x_min, top=x_max)\n",
    "    ax.set_xlabel(\"x_1\")\n",
    "    ax.set_ylabel(\"x_2\")\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.grid(True, alpha=0.25)\n",
    "\n",
    "# Plot the values over the input space\n",
    "plot_model_input_space(model, cartesian_train_dataset, cartesian_test_dataset, x_min=-30, x_max=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that we didn't do anything particularly sophisticated or even what would generally be considered good practice, but through the \"magic\" of deep-learning it paid off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
